{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import string\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "# fname = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "# df = pd.read_sas(fname, 'sas7bdat', encoding=\"ISO-8859-1\")\n",
    "\n",
    "fname = 'immigration_data_sample.csv'\n",
    "df = pd.read_csv(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: integer (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: integer (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df_spark =spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n",
    "df_spark = spark.read.csv(\"immigration_data_sample.csv\", inferSchema=True, header=True)\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#write to parquet\n",
    "df_spark.write.parquet(\"sas_data\")\n",
    "df_spark=spark.read.parquet(\"sas_data\")\n",
    "df_spark.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "cities_df_spark = spark.read.csv(\"us-cities-demographics.csv\",sep=\";\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "fname = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "temprature_df_spark = spark.read.csv(fname, inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Performing cleaning tasks here\n",
    "\n",
    "# cleaning process to get the values of encoded data in the dictionary at 'i94port.txt' and 'i94res.txt'\n",
    "#i94port\n",
    "with open('i94port.txt', 'r') as i94portData:\n",
    "    i94portData = i94portData.readlines()\n",
    "#i94cty\n",
    "with open('i94res.txt', 'r') as i94resData:\n",
    "     i94resData = i94resData.readlines()\n",
    "        \n",
    "#i94port\n",
    "i94portData = {i.translate(str.maketrans('\\t\",', '   ')) for i in i94portData}\n",
    "i94portData = {i.translate(str.maketrans(\"'\", \" \")) for i in i94portData}\n",
    "\n",
    "#i94res\n",
    "i94resData = {i.translate(str.maketrans(\",'\", '  ')) for i in i94resData} \n",
    "   \n",
    "#i94port splitting process\n",
    "i94portData = {k.split('=')[0].strip():k.split('=')[1].strip() for k in i94portData} \n",
    "\n",
    "#i94res splitting process\n",
    "i94resData = {k.split('=')[0].strip():k.split('=')[1].strip() for k in i94resData} \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# transforming the values of 'i94port' and 'i94res' \n",
    "\n",
    "#i94port\n",
    "i94port= df['i94port'].values.tolist()\n",
    "ports = []\n",
    "for i in i94port:\n",
    "    ports.append(i94portData[i])\n",
    "\n",
    "# convert 'i94res' data type to fit the values of the values in the provided data dictionary in 'i94res.txt'\n",
    "df['i94res'] = df['i94res'].astype(int)\n",
    "    \n",
    "#i94cty\n",
    "i94res= df['i94res'].values.tolist()\n",
    "i94res = [str(item) for item in i94res]\n",
    "    \n",
    "ctyRes = []\n",
    "for i in i94res:\n",
    "    ctyRes.append(i94resData[i])\n",
    "        \n",
    "df['i94port'] = ports\n",
    "df['i94res'] = ctyRes\n",
    "    \n",
    "i94DF = df.loc[:, ['cicid','i94port', 'i94res']]\n",
    "i94DF = i94DF.rename(columns = {'i94port': 'port', 'i94res': 'original_residancy'})\n",
    "    \n",
    "i94dfSpark = spark.createDataFrame(i94DF)\n",
    "    \n",
    "# append the transformed data into immigration log dataframes\n",
    "immigration = df_spark.withColumn(\"arrdate\", to_timestamp(col(\"arrdate\")))\\\n",
    "                              .withColumn(\"arrdate\",to_date(col(\"arrdate\")))\\\n",
    "                              .withColumnRenamed('arrdate', 'arrival_date')\\\n",
    "                              .join(i94dfSpark, \"cicid\", \"inner\")\n",
    "immigration_log = immigration.createOrReplaceTempView(\"immigration_log\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write code here\n",
    "\n",
    "#process_cities_demographics_data\n",
    "\n",
    "        #  ---------cities_demographics dimension table ------------\n",
    "\n",
    "cities_demographics_table = cities_df_spark.select('City', 'State Code', 'State','Median Age', \\\n",
    "                                     'Male Population', 'Female Population', 'Total Population'\\\n",
    "                                      , 'Number of Veterans', 'Foreign-born', 'Race')\\\n",
    "                               .withColumnRenamed('State Code', 'State_Code')\\\n",
    "                               .withColumnRenamed('Median Age', 'Median_Age')\\\n",
    "                               .withColumnRenamed('Male Population', 'Male_Population')\\\n",
    "                               .withColumnRenamed('Female Population', 'Female_Population')\\\n",
    "                               .withColumnRenamed('Total Population', 'Total_Population')\\\n",
    "                               .withColumnRenamed('Number of Veterans', 'Number_of_Veterans')\\\n",
    "                               .withColumn('City', upper(col('City')))\\\n",
    "                               .dropDuplicates()\n",
    "    \n",
    "cities_demographics= cities_demographics_table.createOrReplaceTempView(\"cities_demographics\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#process_temprature_data\n",
    "US_weather = temprature_df_spark.filter(temprature_df_spark.Country == \"United States\")\n",
    "    \n",
    "        #  ---------US_weather dimension table ------------\n",
    "    \n",
    "US_weather_table = US_weather.select('dt', 'AverageTemperature','City', 'Latitude', 'Longitude')\\\n",
    "                       .withColumn(\"dt\",to_date(col(\"dt\")))\\\n",
    "                       .withColumn('City', upper(col('City')))\\\n",
    "                       .dropDuplicates()\n",
    "    \n",
    "US_weather = US_weather_table.createOrReplaceTempView(\"US_weather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#process_immigration_data\n",
    "\n",
    " #  ---------i94_card dimension table ------------\n",
    "    \n",
    "i94Card_table = immigration.select('cicid','i94yr', 'i94mon', 'depdate', 'i94visa', \\\n",
    "                          'visatype', 'admnum', 'dtaddto')\\\n",
    "                          .withColumnRenamed('cicid', 'immigration_id')\\\n",
    "                          .withColumn(\"depdate\", to_timestamp(col(\"depdate\")))\\\n",
    "                          .withColumn(\"depdate\",to_date(col(\"depdate\")))\\\n",
    "                          .withColumn(\"dtaddto\", to_timestamp(col(\"dtaddto\")))\\\n",
    "                          .withColumn(\"dtaddto\",to_date(col(\"dtaddto\")))\\\n",
    "                          .withColumnRenamed('i94yr', 'i94year')\\\n",
    "                          .withColumnRenamed('i94mon', 'i94month')\\\n",
    "                          .withColumnRenamed('depdate', 'depature_date')\\\n",
    "                          .withColumnRenamed('admnum', 'admission_number')\\\n",
    "                          .withColumnRenamed('dtaddto', 'admission_date')\\\n",
    "                          .dropDuplicates()\\\n",
    "                          .withColumn(\"i94card_id\", monotonically_increasing_id()) \n",
    "    \n",
    "i94Card = i94Card_table.createOrReplaceTempView(\"i94Card\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    " # ----immigrants dimension table ------------\n",
    "\n",
    "immigrants_table = immigration.select('cicid', 'biryear', 'gender', 'occup','i94visa', 'visatype'\\\n",
    "                                        , 'original_residancy')\\\n",
    "                                    .withColumnRenamed('cicid', 'immigration_id')\\\n",
    "                                    .withColumnRenamed('biryear', 'birth_year')\\\n",
    "                                    .withColumnRenamed('occup', 'occupation')\\\n",
    "                                    .dropDuplicates()\\\n",
    "                                    .withColumn(\"immigrant_id\", monotonically_increasing_id())\n",
    "        \n",
    "immigrants = immigrants_table.createOrReplaceTempView(\"immigrants\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# immigration fact table \n",
    "\n",
    "immigration_fact_table = spark.sql('''\n",
    "              SELECT  \n",
    "              DISTINCT\n",
    "              immigration_log.cicid AS immigration_id,\n",
    "              immigration_log.arrival_date,\n",
    "              immigration_log.i94mode AS immigration_model,\n",
    "              immigration_log.port,\n",
    "              cities_demographics.City,\n",
    "              immigration_log.i94addr AS residancy_state,\n",
    "              i94Card.i94card_id,\n",
    "              immigrants.immigrant_id,\n",
    "              US_weather.AverageTemperature,\n",
    "              US_weather.Latitude,\n",
    "              US_weather.Longitude\n",
    "              FROM  \n",
    "                  immigration_log JOIN i94Card ON immigration_log.cicid == i94Card.immigration_id\n",
    "                  JOIN immigrants ON immigration_log.cicid == immigrants.immigration_id\n",
    "                  LEFT JOIN cities_demographics ON immigration_log.port LIKE '%' || cities_demographics.City || '%'\n",
    "                  LEFT JOIN US_weather ON (US_weather.dt == immigration_log.arrival_date AND \n",
    "                  US_weather.City == cities_demographics.City)\n",
    "                   ''')\n",
    "immigration_fact = immigration_fact_table.createOrReplaceTempView(\"immigration_fact\")\n",
    "     \n",
    "#immigration_fact_table.write.partitionBy('residancy_state','City').parquet('immigration_fact/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+-----------------+--------------+---------+---------------+-------------+-------------+-------------------+--------+---------+\n",
      "|immigration_id|arrival_date|immigration_model|          port|     City|residancy_state|   i94card_id| immigrant_id| AverageTemperature|Latitude|Longitude|\n",
      "+--------------+------------+-----------------+--------------+---------+---------------+-------------+-------------+-------------------+--------+---------+\n",
      "|     3095246.0|  1970-01-01|              1.0|SAN DIEGO   CA|SAN DIEGO|             CA| 429496729603|1262720385025|             13.829|  32.95N|  117.77W|\n",
      "|      861557.0|  1970-01-01|              1.0|SAN DIEGO   CA|SAN DIEGO|           null|1322849927168|  17179869184|             13.829|  32.95N|  117.77W|\n",
      "|     3669540.0|  1970-01-01|              1.0|BALTIMORE   MD|BALTIMORE|           null| 369367187456| 858993459200|-3.6489999999999996|  39.38N|   76.99W|\n",
      "|     5771544.0|  1970-01-01|              1.0|BALTIMORE   MD|BALTIMORE|             MD|1005022347264|1692217114629|-3.6489999999999996|  39.38N|   76.99W|\n",
      "|      298424.0|  1970-01-01|              1.0| NEW YORK   NY| NEW YORK|             NY|1194000908291|1047972020226|             -7.058|  40.99N|   74.56W|\n",
      "+--------------+------------+-----------------+--------------+---------+---------------+-------------+-------------+-------------------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_fact_table.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "immigration_id at immigration_fact_table are duplicated!\n",
      "immigration_id at i94Card_table are unique!\n",
      "immigration_id at immigrants_table are unique!\n"
     ]
    }
   ],
   "source": [
    "# Perform quality checks here\n",
    "#check the uniqueness of immigration_id in immigration_fact_table, i94Card_table, and immigrants_table\n",
    "if (immigration_fact_table.select('immigration_id').count() != immigration_fact_table.select('immigration_id').dropDuplicates().count()):\n",
    "    print(\"immigration_id at immigration_fact_table are duplicated!\")\n",
    "else:\n",
    "        print(\"immigration_id at immigration_table are unique!\")\n",
    "        \n",
    "if (i94Card_table.select('immigration_id').count() != i94Card_table.select('immigration_id').dropDuplicates().count()):\n",
    "    print(\"immigration_id at i94Card_table are duplicated!\")\n",
    "else:\n",
    "    print(\"immigration_id at i94Card_table are unique!\")\n",
    "        \n",
    "if (immigrants_table.select('immigration_id').count() != immigrants_table.select('immigration_id').dropDuplicates().count()):\n",
    "    print(\"immigration_id at immigrants_table are duplicated!\")\n",
    "else:\n",
    "    print(\"immigration_id at immigrants_table are unique!\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The NULL values at immigration_id column of immigration_table are shown below!\n"
     ]
    }
   ],
   "source": [
    "#check missing values of immigration_id in immigration_table, i94Card_table, and immigrants_table\n",
    "   \n",
    "print(\"The NULL values at immigration_id column of immigration_table are shown below!\")\n",
    "print(immigration_fact_table.select(count(when(col('immigration_id').isNull() , immigration_fact_table.immigration_id))).show())\n",
    "        \n",
    "print(\"The NULL values at immigration_id column of i94Card_table are shown below!\")\n",
    "print(i94Card_table.select(count(when(col('immigration_id').isNull() , i94Card_table.immigration_id))).show())\n",
    "        \n",
    "print(\"The NULL values at immigration_id column of immigrants_table are shown below!\")\n",
    "print(immigrants_table.select(count(when(col('immigration_id').isNull() , immigrants_table.immigration_id))).show())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "        \n",
    "#check that all immigration_ids are transformed appropriatly from immigration_log\n",
    "if((i94Card_table.select('immigration_id').count() < 1) or (immigration.select('cicid').count() != i94Card_table.select('immigration_id').count())):\n",
    "    print(str(immigration.select('cicid').count() - i94Card_table.select('immigration_id').count()) + \" rows at i94Card_table did not transformed appropriatly from immigration log!\")\n",
    "        \n",
    "if((immigrants_table.select('immigration_id').count() < 1) or (immigration.select('cicid').count() != immigrants_table.select('immigration_id').count())):\n",
    "    print(str(immigration.select('cicid').count() - immigrants_table.select('immigration_id').count()) +\" rows at i94Card_table did not transformed appropriatly from immigration log!\")\n",
    "\n",
    "if((immigration_fact_table.select('immigration_id').count() < 1) or (immigration.select('cicid').count() != immigration_fact_table.select('immigration_id').count())):\n",
    "    print(str(immigration.select('cicid').count() - i94Card_table.select('immigration_id').count()) + \" rows at i94Card_table did not transformed appropriatly from immigration log!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "---What's the goal? What queries will you want to run? How would Spark or Airflow be incorporated? Why did you choose the model you chose?----\n",
    "\n",
    "\n",
    "The goal of this project is to design a data model for the US immigration to analyze the effect of the US immigration on the overall US economy, that may provided by other external datasets, with observing the immigrants demographics based on residency in each city. \n",
    "\n",
    "The modeled data helps the analyst finding out:\n",
    " - The immigration effect on the US economy.\n",
    " - The frequency of immigration in particular period of time, which help observe the corroloation of immigration and US ecnomy improvement.\n",
    " - The US regions that specific immigrants prefer.\n",
    "\n",
    "In this project, spark is incorporated to model the data based on three resources provided by Udacity Capstone project (immigration dataset, temperature dataset, and US cities demographics). Plus, airflow can be incorporated in order to operate the ETL pipelines automatically as well as monitoring and allowing backfills easily.\n",
    "\n",
    "A star schema is used to be optimized for queries in the US immigration analysis. It's been chosen to model the data in this project due to the availability of its small number of tables and simple paths for fetching the requested data. Moreover, the data is faster to be aggregated in star schema.\n",
    "\n",
    "This includes the following tables:\n",
    "\n",
    "Fact Table:\n",
    "    - immigration_fact - records in immigration data associated with the US cities demographics data and the temprature  data.\n",
    "      immigration_id, arrival_date, immigration_model, port, City, residancy_state, i94card_id, immigrant_id, AverageTemperature, Latitude, Longitude\n",
    "      \n",
    "Dimension Tables:\n",
    "    - i94Card - the i94 card data (it includes data filled out once the immigrant arrive to US)\n",
    "      i94card_id, immigration_id, i94year, i94month, depature_date, i94visa, visatype, admission_number, admission_date\n",
    "      \n",
    "   - immigrants - the US immigrants' data\n",
    "      immigrant_id, immigration_id, birth_year, gender, occupation, i94visa, visatype\n",
    "      \n",
    "   - cities_demographics - the US cities demographics\n",
    "       City, State_Code, Median_Age, Male_Population, Female_Population, Total_Population, Number_of_Veterans, Foreign-born\n",
    "       \n",
    "   - US_weather - the US cities weather based on specific dates\n",
    "       dt, AverageTemperature, City, Latitude, Longitude\n",
    "       \n",
    "** Notes **\n",
    "-The purpose of creating i94Card and immigrants tables is to make the main dataset (immigration dataset) clearer and more organized for analysis. Unfortunately, unique immigrants' ids such as, immigrants' passport numbers or visa numbers are not supported by immigration dataset to make immigrants table more sensible. Therefore, the immigrants ids are created inclemently based on immigration_id (cicid), which causes redundancy on the i94Card_id and immigrants_id.\n",
    "\n",
    "- Another point, all dates in the tables are transformed from 'numbers' into 'date' data types, and they are resulted the same date!!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "--- Clearly state the rationale for the choice of tools and technologies for the project ----\n",
    "\n",
    "In this project, pyspark sql and functions are used for data modeling due to their high quality in sophisticated data analysis and processing. Generally, spark became the most common tool dealing with big data. It enables arbitrary operators for flexible data processing such as, mappers, reducers, joins, group bys, and filters. Also, it considers a faster tool to access and manipulate large datasets."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "---Document the steps of the process---\n",
    "\n",
    "- Create spark session. \n",
    "- Process us_cities_demographics_data to create cities-demographics table\n",
    "- Process_temprature_data to create US-weather table\n",
    "- Process_immigration_data to create i94Card and immigrants tables\n",
    "- Extract columns from joined immigration, us-cities-demographic, and weather datasets to create immigration_fact table \n",
    "- Write the resulted data as parquet files partitioned by 'residancy_state' and 'City'\n",
    "- Check_data_quality by checking the following cases:\n",
    "    1. The uniqueness of immigration_id in immigration_table, i94Card_table, and immigrants_table\n",
    "    2. The missing values of immigration_id in immigration_table, i94Card_table, and immigrants_table\n",
    "    3. Check that all immigration_ids are transformed appropriatly from immigration_log\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "---Propose how often the data should be updated and why.---\n",
    "\n",
    "The data update and scheduling need to be done frequently whenever the data arrival is larger and more frequent. Therefore, I believe data should be updated hourly since the immigration movement rate is changing every single minute due to the frequent immigrants’ arrival around the US. \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "Include a description of how you would approach the problem differently under the following scenarios:\n",
    "\n",
    "******** If the data was increased by 100x **********\n",
    "In this scenario, data size partitioning is necessary since dealing with smaller amount of data is simpler, easier to debug and rerunning tasks as well as faster to be processed. \n",
    "\n",
    "******* If the pipelines were run on a daily basis by 7am ***********\n",
    "In this case, data pipelines scheduling is significant to reduce the amount of data that need to be processed, which contribute to the quality and accuracy of the analyses. The frequency of data arriving and size of data should be considered, where the pipelines need to be scheduled frequently whenever the size of arrived data is larger.\n",
    "\n",
    "****** If the database needed to be accessed by 100+ people ***********\n",
    "In case of accessing data by too many people, a secure and remote access platform should be available especially for companies' employees who works remotely). For additional security, data access credential should be variant from one person to another based on their eligibility determined by the organization.\n",
    "(resource:https://www.helpsystems.com/resources/guides/data-access-big-data-world)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
